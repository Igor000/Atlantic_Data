# Atlantic_Data

The current directory structure has the following sub directories:
python
html
input_data
sql


The main python file is parse_input_file.py
It parses input_data/demo_input_data.txt and creates 2 separated output files:
customer.csv
purchase.csv

These 2 files (customer.csv and purchase.csv) are to be loaded into Postgres db

Inside Postgres db there are 2 tables for these files:
atlantic_customer
atlantic_purchase

The Postgres database schema is in ~sql directory inside this file:
customer_purchase_sql.txt


CREATE TABLE public.atlantic_customer
(
  cust_id integer,
  first_name character(30),
  last_name character(30),
  adrress character(50),
  state character(2),
  zip character(5),
  date_time timestamp with time zone,
  load_time timestamp with time zone
)
WITH (
  OIDS=TRUE
);

CREATE TABLE public.atlantic_purchase
(
  cust_id integer,
  purchase_status character(10),
  product_id integer,
  product_name character(100),
  purchase_amt money,
  date_time time with time zone,
  load_time timestamp with time zone
)
WITH (
  OIDS=TRUE
);


I've tried to normalize our data in the database. All customer related fields (cust_id, name, address) are in
atlantic_customer table.
All purchase information is in atlantic_purchase

To link these 2 table I put field cust_id in the both tables.
In addition to the all exsiting fields in the main input file I've added "load_time" to the each of the tables.
The column "load_time" has a time stamp (generated by Python) when 2 new load files (customer and purchase) are created

The HTML directory has a mock index.html with file upload, parsing/loading, and DB analysis.
We obviously need to use something like Django framework to activate these buttons.

The result of the button "Parse file and Load" will be number of rows loaded into 2 our tables and number of rejected rows.
(atlantic_customer and atlantic_purchase)
If there any rejected rows - it should be investigated.

The button "Postgres DB Analysis" will have the following functionality.
1) We need to check the field atlantic_purchase.purchase_status for its value.
   Now we have only 2 valid values ("new" and "canceled"). Everything else should be identified as invalid.

2) Value "new" should always be inserted before "canceled" - othwerwise it's should be reported as invalid.

3) We need to compare customer names and their addresses for every cust_id.
   If we see any changes in customer names or their addresses it should be investigated.

4) The products with the same product_id should have the same product_names and the same purchase_amt.
   If we see any discrapanies here it should be investigated as well.

5) We can also compare the number of rows loaded last time to the average for the last week and month.
   These numbers should be comparable.

6) Logically we don't need to load any data into atlantic_customer table if they are the same as before.
   We need just verify it and we can remove dublicate rows later.
   
   All these DB analysis can be done using only SQL or it can be combination of Python and SQL


